nohup: ignoring input
2023-05-30 18:07:06.172171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-30 18:07:06.644517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Namespace(adapter=1, audio_feature='chinese-macbert-large-4-UTT', batch_size=16, dataset='MER2023', debug=False, dropout=0.5, epochs=38, gpu=1, l2=1e-05, layers='256,128', lr=0.001, model_type='attention', n_classes=6, num_folder=5, num_workers=0, save_root='./saved-unimodal', savewhole=False, seed=100, test_dataset='MER2023', test_sets=['test3'], text_feature='chinese-macbert-large-4-UTT', train_dataset='MER2023', video_feature='chinese-macbert-large-4-UTT')
====== Reading Data =======
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Step2: training (multiple epoches)
0.0002
epoch:1; eval_acc:0.3423; eval_fscore:0.2453; eval_val_mse:2.9192; eval_metric:-0.4846
0.0004
epoch:2; eval_acc:0.3705; eval_fscore:0.2890; eval_val_mse:2.6338; eval_metric:-0.3695
0.0006
epoch:3; eval_acc:0.3497; eval_fscore:0.2778; eval_val_mse:2.6968; eval_metric:-0.3964
0.0008
epoch:4; eval_acc:0.3557; eval_fscore:0.2468; eval_val_mse:2.7310; eval_metric:-0.4359
0.001
epoch:5; eval_acc:0.3586; eval_fscore:0.3006; eval_val_mse:2.9522; eval_metric:-0.4374
0.001
epoch:6; eval_acc:0.3720; eval_fscore:0.3091; eval_val_mse:2.6365; eval_metric:-0.3501
0.001
epoch:7; eval_acc:0.3705; eval_fscore:0.3301; eval_val_mse:2.6287; eval_metric:-0.3271
0.0009000000000000001
epoch:8; eval_acc:0.3616; eval_fscore:0.3170; eval_val_mse:3.0584; eval_metric:-0.4476
0.0009000000000000001
epoch:9; eval_acc:0.3631; eval_fscore:0.3215; eval_val_mse:3.1232; eval_metric:-0.4593
0.0009000000000000001
epoch:10; eval_acc:0.3720; eval_fscore:0.3444; eval_val_mse:2.8143; eval_metric:-0.3592
0.0008100000000000001
epoch:11; eval_acc:0.3631; eval_fscore:0.3526; eval_val_mse:2.9346; eval_metric:-0.3811
0.0008100000000000001
epoch:12; eval_acc:0.3586; eval_fscore:0.3353; eval_val_mse:2.9229; eval_metric:-0.3954
0.0008100000000000001
epoch:13; eval_acc:0.3795; eval_fscore:0.3631; eval_val_mse:2.9321; eval_metric:-0.3699
0.0007290000000000002
epoch:14; eval_acc:0.3676; eval_fscore:0.3529; eval_val_mse:2.9399; eval_metric:-0.3820
0.0007290000000000002
epoch:15; eval_acc:0.3795; eval_fscore:0.3754; eval_val_mse:2.8036; eval_metric:-0.3255
0.0007290000000000002
epoch:16; eval_acc:0.3705; eval_fscore:0.3648; eval_val_mse:2.7373; eval_metric:-0.3195
0.0006561000000000001
epoch:17; eval_acc:0.3810; eval_fscore:0.3613; eval_val_mse:2.8418; eval_metric:-0.3491
0.0006561000000000001
epoch:18; eval_acc:0.3690; eval_fscore:0.3724; eval_val_mse:2.9312; eval_metric:-0.3604
0.0006561000000000001
epoch:19; eval_acc:0.3571; eval_fscore:0.3608; eval_val_mse:2.6403; eval_metric:-0.2993
0.00059049
epoch:20; eval_acc:0.3661; eval_fscore:0.3625; eval_val_mse:2.9622; eval_metric:-0.3780
0.00059049
epoch:21; eval_acc:0.3438; eval_fscore:0.3428; eval_val_mse:2.8196; eval_metric:-0.3621
0.00059049
epoch:22; eval_acc:0.3661; eval_fscore:0.3648; eval_val_mse:2.9361; eval_metric:-0.3692
0.000531441
epoch:23; eval_acc:0.3750; eval_fscore:0.3725; eval_val_mse:2.8605; eval_metric:-0.3426
0.000531441
epoch:24; eval_acc:0.3676; eval_fscore:0.3690; eval_val_mse:2.9591; eval_metric:-0.3708
0.000531441
epoch:25; eval_acc:0.3631; eval_fscore:0.3617; eval_val_mse:2.7861; eval_metric:-0.3348
0.0004782969000000001
epoch:26; eval_acc:0.3780; eval_fscore:0.3785; eval_val_mse:2.8044; eval_metric:-0.3226
0.0004782969000000001
epoch:27; eval_acc:0.3676; eval_fscore:0.3629; eval_val_mse:2.7946; eval_metric:-0.3357
0.0004782969000000001
epoch:28; eval_acc:0.3601; eval_fscore:0.3601; eval_val_mse:2.7622; eval_metric:-0.3305
0.0004304672100000001
epoch:29; eval_acc:0.3795; eval_fscore:0.3775; eval_val_mse:2.9186; eval_metric:-0.3521
0.0004304672100000001
epoch:30; eval_acc:0.3438; eval_fscore:0.3458; eval_val_mse:2.8457; eval_metric:-0.3656
0.0004304672100000001
epoch:31; eval_acc:0.3452; eval_fscore:0.3470; eval_val_mse:2.8656; eval_metric:-0.3694
0.0003874204890000001
epoch:32; eval_acc:0.3795; eval_fscore:0.3745; eval_val_mse:2.9792; eval_metric:-0.3703
0.0003874204890000001
epoch:33; eval_acc:0.3661; eval_fscore:0.3642; eval_val_mse:2.8602; eval_metric:-0.3508
0.0003874204890000001
epoch:34; eval_acc:0.3780; eval_fscore:0.3757; eval_val_mse:2.9449; eval_metric:-0.3605
0.0003486784401000001
epoch:35; eval_acc:0.3601; eval_fscore:0.3568; eval_val_mse:2.9010; eval_metric:-0.3685
0.0003486784401000001
epoch:36; eval_acc:0.3735; eval_fscore:0.3684; eval_val_mse:2.8973; eval_metric:-0.3560
0.0003486784401000001
epoch:37; eval_acc:0.3690; eval_fscore:0.3663; eval_val_mse:2.8071; eval_metric:-0.3354
0.0003138105960900001
epoch:38; eval_acc:0.3586; eval_fscore:0.3568; eval_val_mse:2.8203; eval_metric:-0.3483
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1 folder, duration: 4254.651562452316 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Step2: training (multiple epoches)
0.0002
epoch:1; eval_acc:0.3080; eval_fscore:0.2216; eval_val_mse:2.5672; eval_metric:-0.4202
0.0004
epoch:2; eval_acc:0.3482; eval_fscore:0.2539; eval_val_mse:2.3882; eval_metric:-0.3431
0.0006
epoch:3; eval_acc:0.3631; eval_fscore:0.2525; eval_val_mse:2.3856; eval_metric:-0.3439
0.0008
epoch:4; eval_acc:0.3616; eval_fscore:0.2939; eval_val_mse:2.3345; eval_metric:-0.2897
0.001
epoch:5; eval_acc:0.3646; eval_fscore:0.2969; eval_val_mse:2.3774; eval_metric:-0.2975
0.001
epoch:6; eval_acc:0.3616; eval_fscore:0.2985; eval_val_mse:2.4607; eval_metric:-0.3167
0.001
epoch:7; eval_acc:0.3705; eval_fscore:0.3136; eval_val_mse:2.6268; eval_metric:-0.3431
0.0009000000000000001
epoch:8; eval_acc:0.3601; eval_fscore:0.3200; eval_val_mse:2.4792; eval_metric:-0.2998
0.0009000000000000001
epoch:9; eval_acc:0.3631; eval_fscore:0.3231; eval_val_mse:2.8781; eval_metric:-0.3964
0.0009000000000000001
epoch:10; eval_acc:0.3824; eval_fscore:0.3291; eval_val_mse:2.9808; eval_metric:-0.4161
0.0008100000000000001
epoch:11; eval_acc:0.3705; eval_fscore:0.3421; eval_val_mse:2.8329; eval_metric:-0.3661
0.0008100000000000001
epoch:12; eval_acc:0.3542; eval_fscore:0.3240; eval_val_mse:2.9433; eval_metric:-0.4118
0.0008100000000000001
epoch:13; eval_acc:0.3542; eval_fscore:0.3228; eval_val_mse:2.6979; eval_metric:-0.3517
0.0007290000000000002
epoch:14; eval_acc:0.3750; eval_fscore:0.3404; eval_val_mse:2.6968; eval_metric:-0.3338
0.0007290000000000002
epoch:15; eval_acc:0.3720; eval_fscore:0.3453; eval_val_mse:2.8359; eval_metric:-0.3637
0.0007290000000000002
epoch:16; eval_acc:0.3780; eval_fscore:0.3477; eval_val_mse:2.6643; eval_metric:-0.3184
0.0006561000000000001
epoch:17; eval_acc:0.3929; eval_fscore:0.3655; eval_val_mse:2.7267; eval_metric:-0.3162
0.0006561000000000001
epoch:18; eval_acc:0.3929; eval_fscore:0.3634; eval_val_mse:2.6808; eval_metric:-0.3068
0.0006561000000000001
epoch:19; eval_acc:0.3720; eval_fscore:0.3561; eval_val_mse:2.7463; eval_metric:-0.3305
0.00059049
epoch:20; eval_acc:0.4018; eval_fscore:0.3914; eval_val_mse:2.8431; eval_metric:-0.3194
0.00059049
epoch:21; eval_acc:0.3765; eval_fscore:0.3553; eval_val_mse:2.7546; eval_metric:-0.3333
0.00059049
epoch:22; eval_acc:0.3899; eval_fscore:0.3664; eval_val_mse:2.7069; eval_metric:-0.3103
0.000531441
epoch:23; eval_acc:0.3854; eval_fscore:0.3675; eval_val_mse:2.7169; eval_metric:-0.3117
0.000531441
epoch:24; eval_acc:0.3929; eval_fscore:0.3843; eval_val_mse:2.8000; eval_metric:-0.3157
0.000531441
epoch:25; eval_acc:0.3869; eval_fscore:0.3763; eval_val_mse:2.7241; eval_metric:-0.3047
0.0004782969000000001
epoch:26; eval_acc:0.4048; eval_fscore:0.3904; eval_val_mse:2.6769; eval_metric:-0.2788
0.0004782969000000001
epoch:27; eval_acc:0.4048; eval_fscore:0.3985; eval_val_mse:2.6441; eval_metric:-0.2626
0.0004782969000000001
epoch:28; eval_acc:0.4033; eval_fscore:0.3918; eval_val_mse:2.7216; eval_metric:-0.2886
0.0004304672100000001
epoch:29; eval_acc:0.3973; eval_fscore:0.3889; eval_val_mse:2.5836; eval_metric:-0.2570
0.0004304672100000001
epoch:30; eval_acc:0.4122; eval_fscore:0.4063; eval_val_mse:2.5593; eval_metric:-0.2335
0.0004304672100000001
epoch:31; eval_acc:0.4137; eval_fscore:0.4040; eval_val_mse:2.7303; eval_metric:-0.2786
0.0003874204890000001
epoch:32; eval_acc:0.3720; eval_fscore:0.3627; eval_val_mse:2.6551; eval_metric:-0.3010
0.0003874204890000001
epoch:33; eval_acc:0.4033; eval_fscore:0.3916; eval_val_mse:2.7242; eval_metric:-0.2894
0.0003874204890000001
epoch:34; eval_acc:0.3869; eval_fscore:0.3799; eval_val_mse:2.6605; eval_metric:-0.2852
0.0003486784401000001
epoch:35; eval_acc:0.4048; eval_fscore:0.3902; eval_val_mse:2.5897; eval_metric:-0.2572
0.0003486784401000001
epoch:36; eval_acc:0.3943; eval_fscore:0.3909; eval_val_mse:2.6455; eval_metric:-0.2705
0.0003486784401000001
epoch:37; eval_acc:0.4003; eval_fscore:0.3934; eval_val_mse:2.5478; eval_metric:-0.2436
0.0003138105960900001
epoch:38; eval_acc:0.4137; eval_fscore:0.4003; eval_val_mse:2.6689; eval_metric:-0.2669
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2 folder, duration: 4264.531911849976 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Step2: training (multiple epoches)
0.0002
epoch:1; eval_acc:0.3065; eval_fscore:0.1993; eval_val_mse:2.5858; eval_metric:-0.4472
0.0004
epoch:2; eval_acc:0.3467; eval_fscore:0.2723; eval_val_mse:2.8673; eval_metric:-0.4445
0.0006
epoch:3; eval_acc:0.3631; eval_fscore:0.2847; eval_val_mse:2.5711; eval_metric:-0.3580
0.0008
epoch:4; eval_acc:0.3318; eval_fscore:0.2395; eval_val_mse:2.8529; eval_metric:-0.4737
0.001
epoch:5; eval_acc:0.3631; eval_fscore:0.2864; eval_val_mse:2.4519; eval_metric:-0.3266
0.001
epoch:6; eval_acc:0.3616; eval_fscore:0.2896; eval_val_mse:2.5006; eval_metric:-0.3355
0.001
epoch:7; eval_acc:0.3571; eval_fscore:0.2729; eval_val_mse:2.4986; eval_metric:-0.3518
0.0009000000000000001
epoch:8; eval_acc:0.3646; eval_fscore:0.2933; eval_val_mse:2.4217; eval_metric:-0.3121
0.0009000000000000001
epoch:9; eval_acc:0.3765; eval_fscore:0.3378; eval_val_mse:2.7342; eval_metric:-0.3457
0.0009000000000000001
epoch:10; eval_acc:0.3690; eval_fscore:0.3215; eval_val_mse:2.5588; eval_metric:-0.3182
0.0008100000000000001
epoch:11; eval_acc:0.3676; eval_fscore:0.3274; eval_val_mse:2.7088; eval_metric:-0.3498
0.0008100000000000001
epoch:12; eval_acc:0.3690; eval_fscore:0.3346; eval_val_mse:2.7374; eval_metric:-0.3497
0.0008100000000000001
epoch:13; eval_acc:0.3914; eval_fscore:0.3556; eval_val_mse:2.5932; eval_metric:-0.2927
0.0007290000000000002
epoch:14; eval_acc:0.3839; eval_fscore:0.3584; eval_val_mse:2.4566; eval_metric:-0.2557
0.0007290000000000002
epoch:15; eval_acc:0.3824; eval_fscore:0.3699; eval_val_mse:2.6666; eval_metric:-0.2967
0.0007290000000000002
epoch:16; eval_acc:0.3750; eval_fscore:0.3712; eval_val_mse:2.6831; eval_metric:-0.2996
0.0006561000000000001
epoch:17; eval_acc:0.3869; eval_fscore:0.3827; eval_val_mse:2.5468; eval_metric:-0.2540
0.0006561000000000001
epoch:18; eval_acc:0.3988; eval_fscore:0.3883; eval_val_mse:2.7806; eval_metric:-0.3069
0.0006561000000000001
epoch:19; eval_acc:0.4062; eval_fscore:0.4048; eval_val_mse:2.6425; eval_metric:-0.2558
0.00059049
epoch:20; eval_acc:0.3780; eval_fscore:0.3802; eval_val_mse:2.5491; eval_metric:-0.2571
0.00059049
epoch:21; eval_acc:0.3943; eval_fscore:0.3899; eval_val_mse:2.6399; eval_metric:-0.2701
0.00059049
epoch:22; eval_acc:0.3988; eval_fscore:0.3980; eval_val_mse:2.6944; eval_metric:-0.2756
0.000531441
epoch:23; eval_acc:0.3854; eval_fscore:0.3843; eval_val_mse:2.7530; eval_metric:-0.3040
0.000531441
epoch:24; eval_acc:0.3810; eval_fscore:0.3745; eval_val_mse:2.6405; eval_metric:-0.2857
0.000531441
epoch:25; eval_acc:0.3884; eval_fscore:0.3879; eval_val_mse:2.5845; eval_metric:-0.2582
0.0004782969000000001
epoch:26; eval_acc:0.3750; eval_fscore:0.3772; eval_val_mse:2.6599; eval_metric:-0.2878
0.0004782969000000001
epoch:27; eval_acc:0.3973; eval_fscore:0.3927; eval_val_mse:2.7528; eval_metric:-0.2955
0.0004782969000000001
epoch:28; eval_acc:0.3765; eval_fscore:0.3796; eval_val_mse:2.6707; eval_metric:-0.2881
0.0004304672100000001
epoch:29; eval_acc:0.3735; eval_fscore:0.3708; eval_val_mse:2.6463; eval_metric:-0.2908
0.0004304672100000001
epoch:30; eval_acc:0.3943; eval_fscore:0.3930; eval_val_mse:2.6452; eval_metric:-0.2683
0.0004304672100000001
epoch:31; eval_acc:0.3958; eval_fscore:0.3926; eval_val_mse:2.5614; eval_metric:-0.2477
0.0003874204890000001
epoch:32; eval_acc:0.3810; eval_fscore:0.3808; eval_val_mse:2.5822; eval_metric:-0.2647
0.0003874204890000001
epoch:33; eval_acc:0.3958; eval_fscore:0.3913; eval_val_mse:2.6586; eval_metric:-0.2733
0.0003874204890000001
epoch:34; eval_acc:0.3914; eval_fscore:0.3769; eval_val_mse:2.6045; eval_metric:-0.2742
0.0003486784401000001
epoch:35; eval_acc:0.3884; eval_fscore:0.3866; eval_val_mse:2.6186; eval_metric:-0.2680
0.0003486784401000001
epoch:36; eval_acc:0.3958; eval_fscore:0.3945; eval_val_mse:2.6436; eval_metric:-0.2664
0.0003486784401000001
epoch:37; eval_acc:0.3958; eval_fscore:0.3927; eval_val_mse:2.6942; eval_metric:-0.2809
0.0003138105960900001
epoch:38; eval_acc:0.3780; eval_fscore:0.3778; eval_val_mse:2.6940; eval_metric:-0.2957
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3 folder, duration: 4260.1957194805145 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Step2: training (multiple epoches)
0.0002
epoch:1; eval_acc:0.3571; eval_fscore:0.2864; eval_val_mse:2.6490; eval_metric:-0.3758
0.0004
epoch:2; eval_acc:0.3943; eval_fscore:0.3362; eval_val_mse:2.7571; eval_metric:-0.3531
0.0006
epoch:3; eval_acc:0.3452; eval_fscore:0.2786; eval_val_mse:2.8991; eval_metric:-0.4462
0.0008
epoch:4; eval_acc:0.3795; eval_fscore:0.3030; eval_val_mse:2.6364; eval_metric:-0.3561
0.001
epoch:5; eval_acc:0.3542; eval_fscore:0.3005; eval_val_mse:2.9257; eval_metric:-0.4310
0.001
epoch:6; eval_acc:0.3690; eval_fscore:0.3074; eval_val_mse:2.5847; eval_metric:-0.3387
0.001
epoch:7; eval_acc:0.3646; eval_fscore:0.3008; eval_val_mse:2.7319; eval_metric:-0.3822
0.0009000000000000001
epoch:8; eval_acc:0.3527; eval_fscore:0.2931; eval_val_mse:3.1809; eval_metric:-0.5021
0.0009000000000000001
epoch:9; eval_acc:0.3586; eval_fscore:0.3168; eval_val_mse:2.9463; eval_metric:-0.4197
0.0009000000000000001
epoch:10; eval_acc:0.3720; eval_fscore:0.3434; eval_val_mse:3.0131; eval_metric:-0.4098
0.0008100000000000001
epoch:11; eval_acc:0.3780; eval_fscore:0.3581; eval_val_mse:3.0716; eval_metric:-0.4098
0.0008100000000000001
epoch:12; eval_acc:0.4048; eval_fscore:0.3868; eval_val_mse:2.8679; eval_metric:-0.3302
0.0008100000000000001
epoch:13; eval_acc:0.3839; eval_fscore:0.3570; eval_val_mse:2.9213; eval_metric:-0.3733
0.0007290000000000002
epoch:14; eval_acc:0.3839; eval_fscore:0.3652; eval_val_mse:2.8700; eval_metric:-0.3523
0.0007290000000000002
epoch:15; eval_acc:0.3869; eval_fscore:0.3702; eval_val_mse:3.2479; eval_metric:-0.4418
0.0007290000000000002
epoch:16; eval_acc:0.3899; eval_fscore:0.3685; eval_val_mse:2.8334; eval_metric:-0.3399
0.0006561000000000001
epoch:17; eval_acc:0.3854; eval_fscore:0.3763; eval_val_mse:2.9159; eval_metric:-0.3527
0.0006561000000000001
epoch:18; eval_acc:0.3929; eval_fscore:0.3930; eval_val_mse:2.9746; eval_metric:-0.3506
0.0006561000000000001
epoch:19; eval_acc:0.3899; eval_fscore:0.3898; eval_val_mse:2.9633; eval_metric:-0.3510
0.00059049
epoch:20; eval_acc:0.4018; eval_fscore:0.3951; eval_val_mse:2.7634; eval_metric:-0.2957
0.00059049
epoch:21; eval_acc:0.4003; eval_fscore:0.3995; eval_val_mse:2.8797; eval_metric:-0.3204
0.00059049
epoch:22; eval_acc:0.3780; eval_fscore:0.3773; eval_val_mse:2.9654; eval_metric:-0.3641
0.000531441
epoch:23; eval_acc:0.3661; eval_fscore:0.3670; eval_val_mse:2.9688; eval_metric:-0.3752
0.000531441
epoch:24; eval_acc:0.4003; eval_fscore:0.4014; eval_val_mse:2.9634; eval_metric:-0.3394
0.000531441
epoch:25; eval_acc:0.3988; eval_fscore:0.3941; eval_val_mse:2.9273; eval_metric:-0.3377
0.0004782969000000001
epoch:26; eval_acc:0.3943; eval_fscore:0.3995; eval_val_mse:2.9241; eval_metric:-0.3315
0.0004782969000000001
epoch:27; eval_acc:0.3824; eval_fscore:0.3805; eval_val_mse:2.9775; eval_metric:-0.3639
0.0004782969000000001
epoch:28; eval_acc:0.3527; eval_fscore:0.3618; eval_val_mse:2.9916; eval_metric:-0.3861
0.0004304672100000001
epoch:29; eval_acc:0.3780; eval_fscore:0.3787; eval_val_mse:2.9330; eval_metric:-0.3545
0.0004304672100000001
epoch:30; eval_acc:0.3720; eval_fscore:0.3754; eval_val_mse:2.9123; eval_metric:-0.3527
0.0004304672100000001
epoch:31; eval_acc:0.3780; eval_fscore:0.3779; eval_val_mse:2.9330; eval_metric:-0.3553
0.0003874204890000001
epoch:32; eval_acc:0.3958; eval_fscore:0.3917; eval_val_mse:2.8965; eval_metric:-0.3325
0.0003874204890000001
epoch:33; eval_acc:0.3705; eval_fscore:0.3728; eval_val_mse:2.7605; eval_metric:-0.3173
0.0003874204890000001
epoch:34; eval_acc:0.3899; eval_fscore:0.3895; eval_val_mse:2.8376; eval_metric:-0.3199
0.0003486784401000001
epoch:35; eval_acc:0.3854; eval_fscore:0.3877; eval_val_mse:2.9315; eval_metric:-0.3452
0.0003486784401000001
epoch:36; eval_acc:0.3869; eval_fscore:0.3883; eval_val_mse:2.9100; eval_metric:-0.3392
0.0003486784401000001
epoch:37; eval_acc:0.3780; eval_fscore:0.3774; eval_val_mse:2.9103; eval_metric:-0.3502
0.0003138105960900001
epoch:38; eval_acc:0.3601; eval_fscore:0.3608; eval_val_mse:2.9804; eval_metric:-0.3843
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4 folder, duration: 4271.547986268997 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
Some weights of the model checkpoint at hfl/chinese-macbert-base were not used when initializing BertAdapterModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Step2: training (multiple epoches)
0.0002
epoch:1; eval_acc:0.3973; eval_fscore:0.2733; eval_val_mse:2.4907; eval_metric:-0.3494
0.0004
epoch:2; eval_acc:0.3854; eval_fscore:0.2677; eval_val_mse:2.6292; eval_metric:-0.3896
0.0006
epoch:3; eval_acc:0.3958; eval_fscore:0.3332; eval_val_mse:2.5396; eval_metric:-0.3017
0.0008
epoch:4; eval_acc:0.3750; eval_fscore:0.3134; eval_val_mse:2.6950; eval_metric:-0.3603
0.001
epoch:5; eval_acc:0.3467; eval_fscore:0.2851; eval_val_mse:2.7509; eval_metric:-0.4027
0.001
epoch:6; eval_acc:0.3795; eval_fscore:0.3218; eval_val_mse:2.6498; eval_metric:-0.3407
0.001
epoch:7; eval_acc:0.3720; eval_fscore:0.3084; eval_val_mse:2.5471; eval_metric:-0.3284
0.0009000000000000001
epoch:8; eval_acc:0.3765; eval_fscore:0.3212; eval_val_mse:2.7599; eval_metric:-0.3688
0.0009000000000000001
epoch:9; eval_acc:0.4018; eval_fscore:0.3555; eval_val_mse:2.7255; eval_metric:-0.3259
0.0009000000000000001
epoch:10; eval_acc:0.3869; eval_fscore:0.3423; eval_val_mse:2.6446; eval_metric:-0.3189
0.0008100000000000001
epoch:11; eval_acc:0.3750; eval_fscore:0.3420; eval_val_mse:2.8443; eval_metric:-0.3691
0.0008100000000000001
epoch:12; eval_acc:0.3676; eval_fscore:0.3377; eval_val_mse:2.7084; eval_metric:-0.3394
0.0008100000000000001
epoch:13; eval_acc:0.3765; eval_fscore:0.3537; eval_val_mse:2.5693; eval_metric:-0.2886
0.0007290000000000002
epoch:14; eval_acc:0.3795; eval_fscore:0.3534; eval_val_mse:2.6974; eval_metric:-0.3210
0.0007290000000000002
epoch:15; eval_acc:0.4077; eval_fscore:0.3746; eval_val_mse:2.7619; eval_metric:-0.3159
0.0007290000000000002
epoch:16; eval_acc:0.3988; eval_fscore:0.3736; eval_val_mse:2.6038; eval_metric:-0.2774
0.0006561000000000001
epoch:17; eval_acc:0.3929; eval_fscore:0.3679; eval_val_mse:2.6542; eval_metric:-0.2957
0.0006561000000000001
epoch:18; eval_acc:0.4077; eval_fscore:0.3753; eval_val_mse:2.6390; eval_metric:-0.2845
0.0006561000000000001
epoch:19; eval_acc:0.4122; eval_fscore:0.4067; eval_val_mse:2.5448; eval_metric:-0.2295
0.00059049
epoch:20; eval_acc:0.4241; eval_fscore:0.4158; eval_val_mse:2.6327; eval_metric:-0.2424
0.00059049
epoch:21; eval_acc:0.4137; eval_fscore:0.4081; eval_val_mse:2.5232; eval_metric:-0.2227
0.00059049
epoch:22; eval_acc:0.4256; eval_fscore:0.4203; eval_val_mse:2.5958; eval_metric:-0.2287
0.000531441
epoch:23; eval_acc:0.4122; eval_fscore:0.4131; eval_val_mse:2.7128; eval_metric:-0.2651
0.000531441
epoch:24; eval_acc:0.3988; eval_fscore:0.3995; eval_val_mse:2.5804; eval_metric:-0.2456
0.000531441
epoch:25; eval_acc:0.4048; eval_fscore:0.4069; eval_val_mse:2.7120; eval_metric:-0.2711
0.0004782969000000001
epoch:26; eval_acc:0.4256; eval_fscore:0.4219; eval_val_mse:2.6970; eval_metric:-0.2524
0.0004782969000000001
epoch:27; eval_acc:0.3973; eval_fscore:0.3993; eval_val_mse:2.5641; eval_metric:-0.2418
0.0004782969000000001
epoch:28; eval_acc:0.4167; eval_fscore:0.4131; eval_val_mse:2.6688; eval_metric:-0.2541
0.0004304672100000001
epoch:29; eval_acc:0.4107; eval_fscore:0.4127; eval_val_mse:2.6639; eval_metric:-0.2533
0.0004304672100000001
epoch:30; eval_acc:0.4137; eval_fscore:0.4126; eval_val_mse:2.7114; eval_metric:-0.2653
0.0004304672100000001
epoch:31; eval_acc:0.4033; eval_fscore:0.4056; eval_val_mse:2.6731; eval_metric:-0.2627
0.0003874204890000001
epoch:32; eval_acc:0.4033; eval_fscore:0.4039; eval_val_mse:2.5853; eval_metric:-0.2424
0.0003874204890000001
epoch:33; eval_acc:0.4256; eval_fscore:0.4247; eval_val_mse:2.5274; eval_metric:-0.2071
0.0003874204890000001
epoch:34; eval_acc:0.3914; eval_fscore:0.3935; eval_val_mse:2.5194; eval_metric:-0.2363
0.0003486784401000001
epoch:35; eval_acc:0.4182; eval_fscore:0.4160; eval_val_mse:2.5228; eval_metric:-0.2147
0.0003486784401000001
epoch:36; eval_acc:0.3869; eval_fscore:0.3884; eval_val_mse:2.4878; eval_metric:-0.2335
0.0003486784401000001
epoch:37; eval_acc:0.4330; eval_fscore:0.4320; eval_val_mse:2.5901; eval_metric:-0.2155
0.0003138105960900001
epoch:38; eval_acc:0.4167; eval_fscore:0.4171; eval_val_mse:2.4725; eval_metric:-0.2010
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5 folder, duration: 4283.292704105377 >>>>>
====== Gain predition on test data =======
