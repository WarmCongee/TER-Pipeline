nohup: ignoring input
Namespace(audio_feature='chinese-macbert-large-4-FRA', batch_size=32, dataset='MER2023_WHISPER_LARGE2_TRANS_-8_-5', debug=False, dropout=0.5, epochs=36, gpu=1, l2=1e-05, layers='256,128', lr=0.001, model_type='attention', n_classes=6, num_folder=5, num_workers=0, save_root='./saved-unimodal', savewhole=False, seed=100, test_dataset='MER2023_WHISPER_LARGE2_TRANS_-8_-5', test_sets=['test3'], text_feature='chinese-macbert-large-4-FRA', train_dataset='MER2023_WHISPER_LARGE2_TRANS_-8_-5', video_feature='chinese-macbert-large-4-FRA')
====== Reading Data =======
0it [00:00, ?it/s]3373it [00:00, 1736301.84it/s]
  0%|          | 0/3373 [00:00<?, ?it/s]  4%|▍         | 129/3373 [00:00<00:02, 1241.99it/s]  8%|▊         | 255/3373 [00:00<00:02, 1234.53it/s] 12%|█▏        | 397/3373 [00:00<00:02, 1301.68it/s] 16%|█▌        | 539/3373 [00:00<00:02, 1336.03it/s] 22%|██▏       | 733/3373 [00:00<00:01, 1550.00it/s] 27%|██▋       | 898/3373 [00:00<00:01, 1582.31it/s] 32%|███▏      | 1069/3373 [00:00<00:01, 1622.77it/s] 37%|███▋      | 1232/3373 [00:00<00:01, 1559.57it/s] 41%|████      | 1391/3373 [00:00<00:01, 1568.65it/s] 49%|████▉     | 1660/3373 [00:01<00:00, 1899.61it/s] 57%|█████▋    | 1934/3373 [00:01<00:00, 2143.16it/s] 64%|██████▎   | 2150/3373 [00:01<00:00, 1912.68it/s] 70%|██████▉   | 2347/3373 [00:01<00:00, 1830.08it/s] 75%|███████▌  | 2534/3373 [00:01<00:00, 1754.77it/s] 80%|████████  | 2713/3373 [00:01<00:00, 1632.42it/s] 85%|████████▌ | 2880/3373 [00:01<00:00, 1631.49it/s] 90%|█████████ | 3046/3373 [00:01<00:00, 1487.83it/s] 96%|█████████▌| 3230/3373 [00:01<00:00, 1576.47it/s]100%|██████████| 3373/3373 [00:02<00:00, 1629.46it/s]
train_frame_1024.py:108: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  feature_dim = np.array(features)[0].shape[-1]
Input feature /home/wyz/MER-TER/TER-Pipeline/features/whisper_old_-8_-5/chinese-macbert-large-4-FRA ===> dim is 1024
0it [00:00, ?it/s]3373it [00:00, 2119776.35it/s]
  0%|          | 0/3373 [00:00<?, ?it/s]  3%|▎         | 105/3373 [00:00<00:03, 1047.31it/s]  6%|▋         | 218/3373 [00:00<00:02, 1086.03it/s] 12%|█▏        | 393/3373 [00:00<00:02, 1372.97it/s] 16%|█▋        | 556/3373 [00:00<00:01, 1471.48it/s] 21%|██        | 707/3373 [00:00<00:01, 1476.78it/s] 26%|██▌       | 861/3373 [00:00<00:01, 1494.25it/s] 30%|██▉       | 1011/3373 [00:00<00:01, 1460.26it/s] 34%|███▍      | 1162/3373 [00:00<00:01, 1457.28it/s] 40%|███▉      | 1336/3373 [00:00<00:01, 1535.61it/s] 44%|████▍     | 1500/3373 [00:01<00:01, 1565.43it/s] 49%|████▉     | 1657/3373 [00:01<00:01, 1558.48it/s] 56%|█████▌    | 1882/3373 [00:01<00:00, 1759.70it/s] 63%|██████▎   | 2138/3373 [00:01<00:00, 1995.26it/s] 69%|██████▉   | 2338/3373 [00:01<00:00, 1505.17it/s] 80%|████████  | 2708/3373 [00:01<00:00, 2039.07it/s] 88%|████████▊ | 2982/3373 [00:01<00:00, 2215.59it/s] 97%|█████████▋| 3261/3373 [00:01<00:00, 2367.71it/s]100%|██████████| 3373/3373 [00:01<00:00, 1798.33it/s]
Input feature /home/wyz/MER-TER/TER-Pipeline/features/whisper_old_-8_-5/chinese-macbert-large-4-FRA ===> dim is 1024
0it [00:00, ?it/s]3373it [00:00, 74870.14it/s]
  0%|          | 0/3373 [00:00<?, ?it/s]  7%|▋         | 251/3373 [00:00<00:01, 2481.40it/s] 16%|█▌        | 523/3373 [00:00<00:01, 2609.59it/s] 23%|██▎       | 789/3373 [00:00<00:00, 2630.90it/s] 31%|███       | 1053/3373 [00:00<00:00, 2473.21it/s] 39%|███▊      | 1302/3373 [00:00<00:01, 1899.28it/s] 46%|████▌     | 1556/3373 [00:00<00:00, 2070.11it/s] 53%|█████▎    | 1784/3373 [00:00<00:00, 2123.07it/s] 61%|██████    | 2041/3373 [00:00<00:00, 2244.25it/s] 68%|██████▊   | 2308/3373 [00:01<00:00, 2367.24it/s] 76%|███████▋  | 2573/3373 [00:01<00:00, 2448.16it/s] 84%|████████▎ | 2824/3373 [00:01<00:00, 2038.61it/s] 90%|█████████ | 3043/3373 [00:01<00:00, 1948.14it/s] 96%|█████████▋| 3248/3373 [00:01<00:00, 1966.70it/s]100%|██████████| 3373/3373 [00:01<00:00, 2079.49it/s]
Input feature /home/wyz/MER-TER/TER-Pipeline/features/whisper_old_-8_-5/chinese-macbert-large-4-FRA ===> dim is 1024
0it [00:00, ?it/s]3373it [00:00, 1037122.45it/s]
  0%|          | 0/3373 [00:00<?, ?it/s]  7%|▋         | 227/3373 [00:00<00:01, 2258.50it/s] 13%|█▎        | 453/3373 [00:00<00:01, 1604.79it/s] 21%|██        | 716/3373 [00:00<00:01, 1978.21it/s] 28%|██▊       | 928/3373 [00:00<00:01, 1973.20it/s] 34%|███▎      | 1134/3373 [00:00<00:01, 1697.00it/s] 41%|████      | 1385/3373 [00:00<00:01, 1926.38it/s] 47%|████▋     | 1589/3373 [00:00<00:01, 1706.13it/s] 53%|█████▎    | 1771/3373 [00:00<00:00, 1720.09it/s] 60%|█████▉    | 2012/3373 [00:01<00:00, 1902.70it/s] 66%|██████▌   | 2210/3373 [00:01<00:00, 1709.02it/s] 71%|███████   | 2390/3373 [00:01<00:00, 1726.77it/s] 79%|███████▉  | 2658/3373 [00:01<00:00, 1983.59it/s] 85%|████████▍ | 2864/3373 [00:01<00:00, 1561.82it/s] 91%|█████████▏| 3084/3373 [00:01<00:00, 1708.48it/s] 98%|█████████▊| 3314/3373 [00:01<00:00, 1842.96it/s]100%|██████████| 3373/3373 [00:01<00:00, 1763.97it/s]
Input feature /home/wyz/MER-TER/TER-Pipeline/features/whisper_old_-8_-5/chinese-macbert-large-4-FRA ===> dim is 1024
0it [00:00, ?it/s]3373it [00:00, 2253127.47it/s]
  0%|          | 0/3373 [00:00<?, ?it/s]  4%|▍         | 147/3373 [00:00<00:02, 1418.12it/s]  9%|▊         | 289/3373 [00:00<00:02, 1279.88it/s] 15%|█▍        | 497/3373 [00:00<00:01, 1619.63it/s] 20%|██        | 684/3373 [00:00<00:01, 1652.69it/s] 25%|██▌       | 851/3373 [00:00<00:01, 1380.64it/s] 32%|███▏      | 1089/3373 [00:00<00:01, 1669.51it/s] 38%|███▊      | 1276/3373 [00:00<00:01, 1705.25it/s] 43%|████▎     | 1454/3373 [00:00<00:01, 1483.87it/s] 51%|█████     | 1704/3373 [00:01<00:00, 1750.93it/s] 56%|█████▌    | 1890/3373 [00:01<00:00, 1681.52it/s] 61%|██████▏   | 2066/3373 [00:01<00:00, 1533.57it/s] 68%|██████▊   | 2284/3373 [00:01<00:00, 1697.50it/s] 73%|███████▎  | 2462/3373 [00:01<00:00, 1523.60it/s] 78%|███████▊  | 2624/3373 [00:01<00:00, 1547.36it/s] 84%|████████▍ | 2845/3373 [00:01<00:00, 1723.18it/s] 90%|████████▉ | 3024/3373 [00:01<00:00, 1691.09it/s] 95%|█████████▍| 3198/3373 [00:02<00:00, 1524.22it/s]100%|██████████| 3373/3373 [00:02<00:00, 1596.55it/s]
Input feature /home/wyz/MER-TER/TER-Pipeline/features/whisper_old_-8_-5/chinese-macbert-large-4-FRA ===> dim is 1024
0it [00:00, ?it/s]3373it [00:00, 4265115.28it/s]
  0%|          | 0/3373 [00:00<?, ?it/s]  3%|▎         | 87/3373 [00:00<00:03, 865.61it/s]  8%|▊         | 266/3373 [00:00<00:02, 1386.87it/s] 12%|█▏        | 405/3373 [00:00<00:02, 1137.14it/s] 19%|█▊        | 626/3373 [00:00<00:01, 1506.55it/s] 24%|██▍       | 803/3373 [00:00<00:01, 1565.79it/s] 29%|██▊       | 965/3373 [00:00<00:01, 1364.10it/s] 35%|███▍      | 1178/3373 [00:00<00:01, 1574.46it/s] 40%|███▉      | 1347/3373 [00:00<00:01, 1585.90it/s] 45%|████▍     | 1512/3373 [00:01<00:01, 1442.63it/s] 52%|█████▏    | 1739/3373 [00:01<00:00, 1662.42it/s] 57%|█████▋    | 1934/3373 [00:01<00:00, 1731.93it/s] 63%|██████▎   | 2113/3373 [00:01<00:00, 1459.62it/s] 70%|██████▉   | 2355/3373 [00:01<00:00, 1696.27it/s] 75%|███████▌  | 2541/3373 [00:01<00:00, 1731.32it/s] 81%|████████  | 2723/3373 [00:01<00:00, 1477.81it/s] 87%|████████▋ | 2928/3373 [00:01<00:00, 1618.59it/s] 92%|█████████▏| 3102/3373 [00:02<00:00, 1557.20it/s] 97%|█████████▋| 3266/3373 [00:02<00:00, 1472.14it/s]100%|██████████| 3373/3373 [00:02<00:00, 1530.85it/s]
Input feature /home/wyz/MER-TER/TER-Pipeline/features/whisper_old_-8_-5/chinese-macbert-large-4-FRA ===> dim is 1024
audio dimension: 1024; text dimension: 1024; video dimension: 1024
====== Training and Evaluation =======
>>>>> Cross-validation: training on the 1 folder >>>>>
Step1: build model (each folder has its own model)
Step2: training (multiple epoches)
epoch:1; eval_acc:0.2753; eval_fscore:0.1359; eval_val_mse:2.9072; eval_metric:-0.5909
epoch:2; eval_acc:0.3601; eval_fscore:0.2612; eval_val_mse:2.5700; eval_metric:-0.3813
epoch:3; eval_acc:0.3705; eval_fscore:0.2852; eval_val_mse:2.2312; eval_metric:-0.2726
epoch:4; eval_acc:0.4077; eval_fscore:0.3431; eval_val_mse:2.2317; eval_metric:-0.2148
epoch:5; eval_acc:0.3929; eval_fscore:0.3180; eval_val_mse:2.3323; eval_metric:-0.2651
epoch:6; eval_acc:0.4196; eval_fscore:0.3838; eval_val_mse:2.2400; eval_metric:-0.1762
epoch:7; eval_acc:0.4405; eval_fscore:0.4151; eval_val_mse:2.2194; eval_metric:-0.1397
epoch:8; eval_acc:0.4524; eval_fscore:0.4309; eval_val_mse:2.2571; eval_metric:-0.1334
epoch:9; eval_acc:0.4449; eval_fscore:0.4264; eval_val_mse:2.5032; eval_metric:-0.1994
epoch:10; eval_acc:0.4360; eval_fscore:0.4152; eval_val_mse:2.5989; eval_metric:-0.2345
epoch:11; eval_acc:0.4256; eval_fscore:0.4166; eval_val_mse:2.8654; eval_metric:-0.2997
epoch:12; eval_acc:0.4211; eval_fscore:0.4167; eval_val_mse:2.7531; eval_metric:-0.2716
epoch:13; eval_acc:0.4286; eval_fscore:0.4155; eval_val_mse:2.6448; eval_metric:-0.2457
epoch:14; eval_acc:0.4092; eval_fscore:0.4014; eval_val_mse:2.9351; eval_metric:-0.3324
epoch:15; eval_acc:0.4107; eval_fscore:0.3987; eval_val_mse:2.7787; eval_metric:-0.2960
epoch:16; eval_acc:0.4092; eval_fscore:0.4027; eval_val_mse:2.8638; eval_metric:-0.3133
epoch:17; eval_acc:0.4003; eval_fscore:0.3973; eval_val_mse:2.7787; eval_metric:-0.2974
epoch:18; eval_acc:0.4033; eval_fscore:0.3944; eval_val_mse:2.8654; eval_metric:-0.3220
epoch:19; eval_acc:0.4122; eval_fscore:0.3992; eval_val_mse:2.7985; eval_metric:-0.3004
epoch:20; eval_acc:0.3914; eval_fscore:0.3904; eval_val_mse:2.7379; eval_metric:-0.2941
epoch:21; eval_acc:0.4018; eval_fscore:0.3963; eval_val_mse:2.8849; eval_metric:-0.3249
epoch:22; eval_acc:0.4092; eval_fscore:0.4001; eval_val_mse:2.8675; eval_metric:-0.3168
epoch:23; eval_acc:0.3943; eval_fscore:0.3900; eval_val_mse:2.7710; eval_metric:-0.3028
epoch:24; eval_acc:0.4003; eval_fscore:0.3935; eval_val_mse:2.7672; eval_metric:-0.2983
epoch:25; eval_acc:0.4092; eval_fscore:0.4022; eval_val_mse:2.8888; eval_metric:-0.3200
epoch:26; eval_acc:0.3973; eval_fscore:0.3970; eval_val_mse:2.8230; eval_metric:-0.3088
epoch:27; eval_acc:0.3988; eval_fscore:0.3931; eval_val_mse:2.7680; eval_metric:-0.2989
epoch:28; eval_acc:0.3973; eval_fscore:0.3930; eval_val_mse:2.7518; eval_metric:-0.2949
epoch:29; eval_acc:0.4062; eval_fscore:0.3993; eval_val_mse:2.7996; eval_metric:-0.3006
epoch:30; eval_acc:0.3988; eval_fscore:0.3954; eval_val_mse:2.7180; eval_metric:-0.2841
epoch:31; eval_acc:0.4062; eval_fscore:0.4026; eval_val_mse:2.7279; eval_metric:-0.2794
epoch:32; eval_acc:0.3854; eval_fscore:0.3817; eval_val_mse:2.7643; eval_metric:-0.3093
epoch:33; eval_acc:0.3839; eval_fscore:0.3811; eval_val_mse:2.7780; eval_metric:-0.3134
epoch:34; eval_acc:0.4062; eval_fscore:0.3982; eval_val_mse:2.7395; eval_metric:-0.2867
epoch:35; eval_acc:0.3869; eval_fscore:0.3822; eval_val_mse:2.7282; eval_metric:-0.2999
epoch:36; eval_acc:0.3869; eval_fscore:0.3840; eval_val_mse:2.7518; eval_metric:-0.3040
Step3: saving and testing on the 1 folder
>>>>> Finish: training on the 1 folder, duration: 5283.218674182892 >>>>>
>>>>> Cross-validation: training on the 2 folder >>>>>
Step1: build model (each folder has its own model)
Step2: training (multiple epoches)
epoch:1; eval_acc:0.2946; eval_fscore:0.2152; eval_val_mse:3.4890; eval_metric:-0.6570
epoch:2; eval_acc:0.3557; eval_fscore:0.2949; eval_val_mse:2.8971; eval_metric:-0.4293
epoch:3; eval_acc:0.4152; eval_fscore:0.3719; eval_val_mse:2.7184; eval_metric:-0.3077
epoch:4; eval_acc:0.4345; eval_fscore:0.4326; eval_val_mse:2.6341; eval_metric:-0.2260
epoch:5; eval_acc:0.3914; eval_fscore:0.3876; eval_val_mse:2.7306; eval_metric:-0.2950
epoch:6; eval_acc:0.4315; eval_fscore:0.4144; eval_val_mse:2.7605; eval_metric:-0.2757
epoch:7; eval_acc:0.4777; eval_fscore:0.4733; eval_val_mse:2.6121; eval_metric:-0.1797
epoch:8; eval_acc:0.4345; eval_fscore:0.4220; eval_val_mse:2.5913; eval_metric:-0.2258
epoch:9; eval_acc:0.4539; eval_fscore:0.4517; eval_val_mse:2.6529; eval_metric:-0.2115
epoch:10; eval_acc:0.4390; eval_fscore:0.4379; eval_val_mse:2.6821; eval_metric:-0.2326
epoch:11; eval_acc:0.4628; eval_fscore:0.4626; eval_val_mse:2.8042; eval_metric:-0.2385
epoch:12; eval_acc:0.4375; eval_fscore:0.4344; eval_val_mse:2.8722; eval_metric:-0.2837
epoch:13; eval_acc:0.4211; eval_fscore:0.4248; eval_val_mse:2.8064; eval_metric:-0.2769
epoch:14; eval_acc:0.4271; eval_fscore:0.4152; eval_val_mse:2.7832; eval_metric:-0.2806
epoch:15; eval_acc:0.4315; eval_fscore:0.4284; eval_val_mse:3.0454; eval_metric:-0.3330
epoch:16; eval_acc:0.4152; eval_fscore:0.4077; eval_val_mse:2.8525; eval_metric:-0.3055
epoch:17; eval_acc:0.4167; eval_fscore:0.4066; eval_val_mse:2.9044; eval_metric:-0.3196
epoch:18; eval_acc:0.4003; eval_fscore:0.3974; eval_val_mse:2.9712; eval_metric:-0.3454
epoch:19; eval_acc:0.4048; eval_fscore:0.4027; eval_val_mse:2.8878; eval_metric:-0.3193
epoch:20; eval_acc:0.4033; eval_fscore:0.4024; eval_val_mse:2.9864; eval_metric:-0.3442
epoch:21; eval_acc:0.4048; eval_fscore:0.4024; eval_val_mse:2.9804; eval_metric:-0.3427
epoch:22; eval_acc:0.4092; eval_fscore:0.4076; eval_val_mse:2.9041; eval_metric:-0.3184
epoch:23; eval_acc:0.4092; eval_fscore:0.4082; eval_val_mse:2.9114; eval_metric:-0.3197
epoch:24; eval_acc:0.4062; eval_fscore:0.4052; eval_val_mse:2.9184; eval_metric:-0.3244
epoch:25; eval_acc:0.4033; eval_fscore:0.3971; eval_val_mse:2.9264; eval_metric:-0.3345
epoch:26; eval_acc:0.4182; eval_fscore:0.4157; eval_val_mse:2.9824; eval_metric:-0.3299
epoch:27; eval_acc:0.4077; eval_fscore:0.4083; eval_val_mse:2.9208; eval_metric:-0.3219
epoch:28; eval_acc:0.4018; eval_fscore:0.4032; eval_val_mse:2.9927; eval_metric:-0.3450
epoch:29; eval_acc:0.4122; eval_fscore:0.4112; eval_val_mse:2.8949; eval_metric:-0.3125
epoch:30; eval_acc:0.4033; eval_fscore:0.4042; eval_val_mse:2.9445; eval_metric:-0.3319
epoch:31; eval_acc:0.4033; eval_fscore:0.4028; eval_val_mse:2.9938; eval_metric:-0.3457
epoch:32; eval_acc:0.3973; eval_fscore:0.3978; eval_val_mse:2.9694; eval_metric:-0.3445
epoch:33; eval_acc:0.4122; eval_fscore:0.4106; eval_val_mse:2.8956; eval_metric:-0.3133
epoch:34; eval_acc:0.4122; eval_fscore:0.4134; eval_val_mse:2.9603; eval_metric:-0.3267
epoch:35; eval_acc:0.3973; eval_fscore:0.3965; eval_val_mse:2.9623; eval_metric:-0.3440
epoch:36; eval_acc:0.3988; eval_fscore:0.4002; eval_val_mse:2.9660; eval_metric:-0.3413
Step3: saving and testing on the 2 folder
>>>>> Finish: training on the 2 folder, duration: 5371.412611484528 >>>>>
>>>>> Cross-validation: training on the 3 folder >>>>>
Step1: build model (each folder has its own model)
Step2: training (multiple epoches)
epoch:1; eval_acc:0.2589; eval_fscore:0.1066; eval_val_mse:3.4923; eval_metric:-0.7664
epoch:2; eval_acc:0.3631; eval_fscore:0.2578; eval_val_mse:3.1535; eval_metric:-0.5306
epoch:3; eval_acc:0.4182; eval_fscore:0.3745; eval_val_mse:2.9271; eval_metric:-0.3573
epoch:4; eval_acc:0.4583; eval_fscore:0.4352; eval_val_mse:2.4633; eval_metric:-0.1806
epoch:5; eval_acc:0.4792; eval_fscore:0.4527; eval_val_mse:2.6305; eval_metric:-0.2049
epoch:6; eval_acc:0.4970; eval_fscore:0.4869; eval_val_mse:2.3799; eval_metric:-0.1081
epoch:7; eval_acc:0.4643; eval_fscore:0.4415; eval_val_mse:2.4843; eval_metric:-0.1796
epoch:8; eval_acc:0.4702; eval_fscore:0.4566; eval_val_mse:2.4464; eval_metric:-0.1550
epoch:9; eval_acc:0.4643; eval_fscore:0.4551; eval_val_mse:2.4284; eval_metric:-0.1520
epoch:10; eval_acc:0.4405; eval_fscore:0.4274; eval_val_mse:2.6094; eval_metric:-0.2249
epoch:11; eval_acc:0.4762; eval_fscore:0.4618; eval_val_mse:2.6777; eval_metric:-0.2076
epoch:12; eval_acc:0.4598; eval_fscore:0.4441; eval_val_mse:2.4989; eval_metric:-0.1806
epoch:13; eval_acc:0.4524; eval_fscore:0.4528; eval_val_mse:2.7473; eval_metric:-0.2340
epoch:14; eval_acc:0.4390; eval_fscore:0.4359; eval_val_mse:2.7154; eval_metric:-0.2430
epoch:15; eval_acc:0.4464; eval_fscore:0.4389; eval_val_mse:2.7229; eval_metric:-0.2418
epoch:16; eval_acc:0.4390; eval_fscore:0.4310; eval_val_mse:2.7621; eval_metric:-0.2596
epoch:17; eval_acc:0.4315; eval_fscore:0.4316; eval_val_mse:2.7695; eval_metric:-0.2608
epoch:18; eval_acc:0.4345; eval_fscore:0.4303; eval_val_mse:2.7608; eval_metric:-0.2599
epoch:19; eval_acc:0.4301; eval_fscore:0.4293; eval_val_mse:2.8749; eval_metric:-0.2895
epoch:20; eval_acc:0.4226; eval_fscore:0.4175; eval_val_mse:2.8577; eval_metric:-0.2969
epoch:21; eval_acc:0.4330; eval_fscore:0.4312; eval_val_mse:2.7573; eval_metric:-0.2581
epoch:22; eval_acc:0.4286; eval_fscore:0.4218; eval_val_mse:2.7808; eval_metric:-0.2734
epoch:23; eval_acc:0.4315; eval_fscore:0.4225; eval_val_mse:2.7932; eval_metric:-0.2758
epoch:24; eval_acc:0.4286; eval_fscore:0.4270; eval_val_mse:2.8324; eval_metric:-0.2811
epoch:25; eval_acc:0.4241; eval_fscore:0.4174; eval_val_mse:2.7301; eval_metric:-0.2651
epoch:26; eval_acc:0.4211; eval_fscore:0.4151; eval_val_mse:2.7527; eval_metric:-0.2731
epoch:27; eval_acc:0.4256; eval_fscore:0.4284; eval_val_mse:2.7788; eval_metric:-0.2663
epoch:28; eval_acc:0.4256; eval_fscore:0.4243; eval_val_mse:2.7432; eval_metric:-0.2615
epoch:29; eval_acc:0.4315; eval_fscore:0.4301; eval_val_mse:2.8202; eval_metric:-0.2750
epoch:30; eval_acc:0.4301; eval_fscore:0.4279; eval_val_mse:2.7260; eval_metric:-0.2536
epoch:31; eval_acc:0.4256; eval_fscore:0.4241; eval_val_mse:2.7565; eval_metric:-0.2650
epoch:32; eval_acc:0.4390; eval_fscore:0.4361; eval_val_mse:2.8381; eval_metric:-0.2735
epoch:33; eval_acc:0.4211; eval_fscore:0.4192; eval_val_mse:2.8189; eval_metric:-0.2856
epoch:34; eval_acc:0.4271; eval_fscore:0.4282; eval_val_mse:2.7999; eval_metric:-0.2718
epoch:35; eval_acc:0.4241; eval_fscore:0.4219; eval_val_mse:2.8168; eval_metric:-0.2823
epoch:36; eval_acc:0.4286; eval_fscore:0.4284; eval_val_mse:2.7492; eval_metric:-0.2589
Step3: saving and testing on the 3 folder
>>>>> Finish: training on the 3 folder, duration: 5342.933573722839 >>>>>
>>>>> Cross-validation: training on the 4 folder >>>>>
Step1: build model (each folder has its own model)
Step2: training (multiple epoches)
epoch:1; eval_acc:0.3065; eval_fscore:0.2118; eval_val_mse:3.1163; eval_metric:-0.5673
epoch:2; eval_acc:0.4256; eval_fscore:0.3807; eval_val_mse:2.6247; eval_metric:-0.2755
epoch:3; eval_acc:0.4018; eval_fscore:0.3534; eval_val_mse:2.3771; eval_metric:-0.2409
epoch:4; eval_acc:0.4048; eval_fscore:0.3559; eval_val_mse:2.2763; eval_metric:-0.2132
epoch:5; eval_acc:0.4271; eval_fscore:0.4092; eval_val_mse:2.5152; eval_metric:-0.2196
epoch:6; eval_acc:0.4643; eval_fscore:0.4374; eval_val_mse:2.3757; eval_metric:-0.1565
epoch:7; eval_acc:0.4628; eval_fscore:0.4428; eval_val_mse:2.2971; eval_metric:-0.1314
epoch:8; eval_acc:0.4524; eval_fscore:0.4307; eval_val_mse:2.3707; eval_metric:-0.1619
epoch:9; eval_acc:0.4479; eval_fscore:0.4340; eval_val_mse:2.4069; eval_metric:-0.1678
epoch:10; eval_acc:0.4435; eval_fscore:0.4273; eval_val_mse:2.3529; eval_metric:-0.1609
epoch:11; eval_acc:0.3899; eval_fscore:0.3791; eval_val_mse:2.3401; eval_metric:-0.2059
epoch:12; eval_acc:0.4345; eval_fscore:0.4206; eval_val_mse:2.3774; eval_metric:-0.1738
epoch:13; eval_acc:0.4315; eval_fscore:0.4266; eval_val_mse:2.5220; eval_metric:-0.2039
epoch:14; eval_acc:0.4420; eval_fscore:0.4258; eval_val_mse:2.5597; eval_metric:-0.2141
epoch:15; eval_acc:0.4226; eval_fscore:0.4143; eval_val_mse:2.6786; eval_metric:-0.2553
epoch:16; eval_acc:0.4122; eval_fscore:0.4032; eval_val_mse:2.6183; eval_metric:-0.2513
epoch:17; eval_acc:0.4211; eval_fscore:0.4126; eval_val_mse:2.5640; eval_metric:-0.2284
epoch:18; eval_acc:0.4226; eval_fscore:0.4118; eval_val_mse:2.7116; eval_metric:-0.2661
epoch:19; eval_acc:0.4137; eval_fscore:0.4058; eval_val_mse:2.6200; eval_metric:-0.2492
epoch:20; eval_acc:0.4241; eval_fscore:0.4170; eval_val_mse:2.5453; eval_metric:-0.2193
epoch:21; eval_acc:0.4048; eval_fscore:0.3997; eval_val_mse:2.5380; eval_metric:-0.2348
epoch:22; eval_acc:0.4122; eval_fscore:0.4049; eval_val_mse:2.6374; eval_metric:-0.2544
epoch:23; eval_acc:0.4196; eval_fscore:0.4123; eval_val_mse:2.5549; eval_metric:-0.2265
epoch:24; eval_acc:0.4241; eval_fscore:0.4159; eval_val_mse:2.6004; eval_metric:-0.2342
epoch:25; eval_acc:0.4062; eval_fscore:0.4022; eval_val_mse:2.5941; eval_metric:-0.2463
epoch:26; eval_acc:0.4241; eval_fscore:0.4162; eval_val_mse:2.5818; eval_metric:-0.2292
epoch:27; eval_acc:0.4226; eval_fscore:0.4150; eval_val_mse:2.5406; eval_metric:-0.2202
epoch:28; eval_acc:0.4182; eval_fscore:0.4068; eval_val_mse:2.6353; eval_metric:-0.2520
epoch:29; eval_acc:0.4226; eval_fscore:0.4177; eval_val_mse:2.5682; eval_metric:-0.2243
epoch:30; eval_acc:0.4196; eval_fscore:0.4149; eval_val_mse:2.6039; eval_metric:-0.2361
epoch:31; eval_acc:0.4286; eval_fscore:0.4224; eval_val_mse:2.5736; eval_metric:-0.2210
epoch:32; eval_acc:0.4048; eval_fscore:0.4002; eval_val_mse:2.5299; eval_metric:-0.2323
epoch:33; eval_acc:0.4092; eval_fscore:0.3991; eval_val_mse:2.5539; eval_metric:-0.2393
epoch:34; eval_acc:0.4077; eval_fscore:0.4011; eval_val_mse:2.5897; eval_metric:-0.2463
epoch:35; eval_acc:0.4167; eval_fscore:0.4104; eval_val_mse:2.5293; eval_metric:-0.2219
epoch:36; eval_acc:0.4122; eval_fscore:0.4078; eval_val_mse:2.6084; eval_metric:-0.2443
Step3: saving and testing on the 4 folder
>>>>> Finish: training on the 4 folder, duration: 5385.297116279602 >>>>>
>>>>> Cross-validation: training on the 5 folder >>>>>
Step1: build model (each folder has its own model)
Step2: training (multiple epoches)
epoch:1; eval_acc:0.2723; eval_fscore:0.1248; eval_val_mse:3.1999; eval_metric:-0.6752
epoch:2; eval_acc:0.3199; eval_fscore:0.2097; eval_val_mse:2.7282; eval_metric:-0.4723
epoch:3; eval_acc:0.3795; eval_fscore:0.3171; eval_val_mse:2.7611; eval_metric:-0.3731
epoch:4; eval_acc:0.4137; eval_fscore:0.3535; eval_val_mse:2.5600; eval_metric:-0.2865
epoch:5; eval_acc:0.4420; eval_fscore:0.4095; eval_val_mse:2.5238; eval_metric:-0.2214
epoch:6; eval_acc:0.4211; eval_fscore:0.3931; eval_val_mse:2.3014; eval_metric:-0.1823
epoch:7; eval_acc:0.4345; eval_fscore:0.4031; eval_val_mse:2.3842; eval_metric:-0.1929
epoch:8; eval_acc:0.4375; eval_fscore:0.4172; eval_val_mse:2.2815; eval_metric:-0.1532
epoch:9; eval_acc:0.4643; eval_fscore:0.4511; eval_val_mse:2.3547; eval_metric:-0.1375
epoch:10; eval_acc:0.4509; eval_fscore:0.4370; eval_val_mse:2.4731; eval_metric:-0.1813
epoch:11; eval_acc:0.4420; eval_fscore:0.4292; eval_val_mse:2.4872; eval_metric:-0.1926
epoch:12; eval_acc:0.4479; eval_fscore:0.4351; eval_val_mse:2.6773; eval_metric:-0.2342
epoch:13; eval_acc:0.4301; eval_fscore:0.4232; eval_val_mse:2.6715; eval_metric:-0.2447
epoch:14; eval_acc:0.4241; eval_fscore:0.4186; eval_val_mse:2.9246; eval_metric:-0.3125
epoch:15; eval_acc:0.4226; eval_fscore:0.4163; eval_val_mse:2.8401; eval_metric:-0.2937
epoch:16; eval_acc:0.4479; eval_fscore:0.4366; eval_val_mse:2.6181; eval_metric:-0.2180
epoch:17; eval_acc:0.4420; eval_fscore:0.4340; eval_val_mse:2.7158; eval_metric:-0.2450
epoch:18; eval_acc:0.4226; eval_fscore:0.4187; eval_val_mse:2.7863; eval_metric:-0.2778
epoch:19; eval_acc:0.4196; eval_fscore:0.4198; eval_val_mse:2.8247; eval_metric:-0.2864
epoch:20; eval_acc:0.4182; eval_fscore:0.4112; eval_val_mse:2.7994; eval_metric:-0.2886
epoch:21; eval_acc:0.4286; eval_fscore:0.4235; eval_val_mse:2.7394; eval_metric:-0.2614
epoch:22; eval_acc:0.4420; eval_fscore:0.4394; eval_val_mse:2.7401; eval_metric:-0.2456
epoch:23; eval_acc:0.4196; eval_fscore:0.4180; eval_val_mse:2.7229; eval_metric:-0.2628
epoch:24; eval_acc:0.4226; eval_fscore:0.4165; eval_val_mse:2.7342; eval_metric:-0.2671
epoch:25; eval_acc:0.4330; eval_fscore:0.4289; eval_val_mse:2.7907; eval_metric:-0.2687
epoch:26; eval_acc:0.4405; eval_fscore:0.4360; eval_val_mse:2.7216; eval_metric:-0.2444
epoch:27; eval_acc:0.4315; eval_fscore:0.4267; eval_val_mse:2.8438; eval_metric:-0.2843
epoch:28; eval_acc:0.4375; eval_fscore:0.4351; eval_val_mse:2.7455; eval_metric:-0.2513
epoch:29; eval_acc:0.4375; eval_fscore:0.4320; eval_val_mse:2.8085; eval_metric:-0.2701
epoch:30; eval_acc:0.4256; eval_fscore:0.4236; eval_val_mse:2.7349; eval_metric:-0.2601
epoch:31; eval_acc:0.4420; eval_fscore:0.4341; eval_val_mse:2.7038; eval_metric:-0.2418
epoch:32; eval_acc:0.4286; eval_fscore:0.4257; eval_val_mse:2.7956; eval_metric:-0.2732
epoch:33; eval_acc:0.4301; eval_fscore:0.4265; eval_val_mse:2.7492; eval_metric:-0.2608
epoch:34; eval_acc:0.4464; eval_fscore:0.4423; eval_val_mse:2.6770; eval_metric:-0.2270
epoch:35; eval_acc:0.4375; eval_fscore:0.4341; eval_val_mse:2.7765; eval_metric:-0.2600
epoch:36; eval_acc:0.4479; eval_fscore:0.4413; eval_val_mse:2.8002; eval_metric:-0.2588
Step3: saving and testing on the 5 folder
>>>>> Finish: training on the 5 folder, duration: 4808.48840212822 >>>>>
====== Gain predition on test data =======
save results in ./saved-unimodal/model/cv_features:chinese-macbert-large-4-FRA+chinese-macbert-large-4-FRA+chinese-macbert-large-4-FRA_f1:0.4570_valmse:2.3802_metric:-0.1380_1685883560.0639234.npz
Traceback (most recent call last):
  File "train_frame_1024.py", line 829, in <module>
    analyzing_asr_impact(folder_save, config.PATH_TO_LABEL[args.train_dataset], config.PATH_TO_TRANSCRIPTIONS[args.train_dataset], emo2idx, idx2emo, args)
KeyError: 'MER2023_WHISPER_LARGE2_TRANS_-8_-5'
